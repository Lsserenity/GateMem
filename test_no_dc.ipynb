{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342b1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"minGPT\")   # 让 mingpt 这个包可 import\n",
    "\n",
    "import torch\n",
    "from minGPT.mingpt.model import GPT\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "from minGPT.mingpt.bpe import BPETokenizer\n",
    "\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed1c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "GradScaler enabled: False\n"
     ]
    }
   ],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For this torch version, use torch.cuda.amp.GradScaler (newer torch supports torch.amp.GradScaler)\n",
    "scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
    "print(f\"GradScaler enabled: {scaler is not None}\")\n",
    "\n",
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe45341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    }
   ],
   "source": [
    "# 调用minGPT\n",
    "# 如果要关掉记忆模块，令types=None即可\n",
    "model, _ = GPT.from_pretrained(model_type, types=\"nm\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074b27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状： torch.Size([32, 1024])\n",
      "目标形状： torch.Size([32, 1024])\n",
      "prompt的类型 <class 'list'> 32\n",
      "prompt的第15个元素： Task: Reverse the string and remove vowels (a,e,i,o,u).\n",
      "Input: wcrwbjdqprjw\n",
      "Output:\n",
      "answer的类型 <class 'list'> 32\n",
      "answer的第15个元素： wjrpqdjbwrcw\n"
     ]
    }
   ],
   "source": [
    "from minGPT.mingpt.program_dataset import ProgramDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dynamic_cheatsheet import DynamicCheatsheetMemory\n",
    "from dynamic_cheatsheet.config_loader import load_config\n",
    "from dataclasses import asdict\n",
    "import random\n",
    "\n",
    "# 准备数据集\n",
    "train_dataset = ProgramDataset(\n",
    "    jsonl_path=\"./data_reverse_dropvowel/train.jsonl\",\n",
    "    block_size=1024,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 得到一个batch的数据\n",
    "def get_one_batch(dataset, batch_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    prompts = []\n",
    "    answers = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        i = random.randrange(len(dataset))\n",
    "        idx, target, prompt, answer = dataset[i]\n",
    "        x.append(idx)\n",
    "        y.append(target)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    \n",
    "    x = torch.stack(x, dim=0)\n",
    "    y = torch.stack(y, dim=0)\n",
    "    \n",
    "    return x, y, prompts, answers\n",
    "\n",
    "x, y, prompts, answers = get_one_batch(train_dataset, batch_size=32)\n",
    "\n",
    "print(\"输入形状：\", x.shape)\n",
    "print(\"目标形状：\", y.shape)\n",
    "print(\"prompt的类型\", type(prompts), len(prompts))\n",
    "print(\"prompt的第15个元素：\", prompts[15])\n",
    "print(\"answer的类型\", type(answers), len(answers))\n",
    "print(\"answer的第15个元素：\", answers[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50345614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果评估函数\n",
    "import Levenshtein\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> float:\n",
    "    \"\"\"\n",
    "    完全匹配：一模一样返回 1.0，否则 0.0\n",
    "    \"\"\"\n",
    "    return float(pred.strip() == gold.strip())\n",
    "\n",
    "def normalized_edit_similarity(pred: str, gold: str) -> float:\n",
    "    \"\"\"\n",
    "    1 - normalized edit distance\n",
    "    取值 [0,1]，1 表示完全相同\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return float(len(pred) == 0)\n",
    "\n",
    "    dist = Levenshtein.distance(pred, gold)\n",
    "    return 1.0 - dist / max(len(pred), len(gold))\n",
    "\n",
    "VOWELS = set(\"aeiouAEIOU\")\n",
    "\n",
    "def remove_vowels(s: str) -> str:\n",
    "    return \"\".join(c for c in s if c not in VOWELS)\n",
    "\n",
    "def reverse_string(s: str) -> str:\n",
    "    return s[::-1]\n",
    "\n",
    "def vowel_removal_accuracy(pred: str) -> float:\n",
    "    \"\"\"\n",
    "    预测中是否完全不包含元音\n",
    "    \"\"\"\n",
    "    return float(all(c not in VOWELS for c in pred))\n",
    "\n",
    "def reverse_consistency(pred: str, gold: str) -> float:\n",
    "    \"\"\"\n",
    "    判断 pred 是否等于 gold 的 reverse\n",
    "    对你这个任务主要用于 debug\n",
    "    \"\"\"\n",
    "    return float(pred == gold[::-1])\n",
    "\n",
    "def evaluate_answer(pred: str, gold: str) -> dict:\n",
    "    pred = pred.strip()\n",
    "    gold = gold.strip()\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": exact_match(pred, gold),\n",
    "        \"edit_similarity\": normalized_edit_similarity(pred, gold),\n",
    "        \"no_vowel\": vowel_removal_accuracy(pred),\n",
    "        \"pred_len\": len(pred),\n",
    "        \"gold_len\": len(gold),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_batch(preds, golds):\n",
    "    all_metrics = []\n",
    "    for p, g in zip(preds, golds):\n",
    "        all_metrics.append(evaluate_answer(p, g))\n",
    "\n",
    "    # 求平均\n",
    "    avg = {}\n",
    "    for k in all_metrics[0]:\n",
    "        avg[k] = sum(m[k] for m in all_metrics) / len(all_metrics)\n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae81c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\29072\\AppData\\Local\\Temp\\ipykernel_20644\\649240920.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "c:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\29072\\AppData\\Local\\Temp\\ipykernel_20644\\649240920.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 0，损失 5.8563\n",
      "迭代 0，评估指标: {'exact_match': 0.0, 'edit_similarity': 0.1322375707695998, 'no_vowel': 0.0, 'pred_len': 38.3, 'gold_len': 7.5}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast():\n\u001b[32m     37\u001b[39m     logits, loss = model(x, y)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m scaler.step(optimizer)\n\u001b[32m     40\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 冻结部分参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "if getattr(model, \"neural_memory\", None) is not None:\n",
    "    for p in model.neural_memory.parameters():\n",
    "        p.requires_grad = True\n",
    "if hasattr(model, \"dc_gate\"):\n",
    "    for p in model.dc_gate.parameters():\n",
    "        p.requires_grad = True\n",
    "if hasattr(model, \"nm_gate\"):\n",
    "    for p in model.nm_gate.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# 获取可训练参数及其名称\n",
    "trainable_params = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "# print(\"Trainable param groups:\")\n",
    "# for n, p in trainable_params:\n",
    "#     print(f\"  {n:60s} {p.numel()}\")\n",
    "# print(\"Total trainable:\", sum(p.numel() for _, p in trainable_params))\n",
    "\n",
    "# 只传递参数对象给优化器\n",
    "optimizer = torch.optim.AdamW([p for _, p in trainable_params], lr=3e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "max_iters = 1000\n",
    "batch_size = 10\n",
    "EOS_ID = 50256\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    x, y, prompts, answers = get_one_batch(train_dataset, batch_size=batch_size)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        logits, loss = model(x, y)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # 开启评估模式，生成文本并更新记忆\n",
    "    model.eval()\n",
    "    gen = []\n",
    "    for i in range(batch_size):\n",
    "        prompt_idx = tokenizer(prompts[i])[0].long().unsqueeze(0).to(device)\n",
    "        gen_ids = model.generate(\n",
    "            prompt_idx,\n",
    "            max_new_tokens=20,\n",
    "            temperature=1.0,\n",
    "            do_sample=False,\n",
    "            top_k=None,\n",
    "            eos_token_id=EOS_ID,\n",
    "            return_only_generated=True,\n",
    "        )\n",
    "        gen_text = tokenizer.decode(gen_ids[0])\n",
    "        gen.append(gen_text)\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"迭代 {iter}，损失 {loss.item():.4f}\")\n",
    "        metrics = evaluate_batch(gen, answers)\n",
    "        print(f\"迭代 {iter}，评估指标: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "\n",
    "# 准备数据集\n",
    "test_dataset = ProgramDataset(\n",
    "    jsonl_path=\"./data_reverse_dropvowel/test.jsonl\",\n",
    "    block_size=1024,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 得到一个batch的数据\n",
    "test_size = 100\n",
    "x, y, prompts, answers = get_one_batch(test_dataset, batch_size=test_size)\n",
    "gen = []\n",
    "\n",
    "for i in range(test_size):\n",
    "\n",
    "    prompt_idx = tokenizer(prompts[i])[0].long().unsqueeze(0).to(device)\n",
    "    gen_ids = model.generate(\n",
    "        prompt_idx,\n",
    "        max_new_tokens=20,\n",
    "        temperature=1.0,\n",
    "        do_sample=False,\n",
    "        top_k=None,\n",
    "        eos_token_id=EOS_ID,\n",
    "        return_only_generated=True,\n",
    "    )\n",
    "    gen_text = tokenizer.decode(gen_ids[0])\n",
    "    gen.append(gen_text)\n",
    "\n",
    "metrics = evaluate_batch(gen, answers)\n",
    "print(f\"test评估指标: {metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
