{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a417d120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\meisai\\envs\\PRML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"minGPT\")   # 让 mingpt 这个包可 import\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from minGPT.mingpt.model import GPT\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66067e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer_hf = GPT2Tokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1511554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (neural_memory): NeuralMemory(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (silu): SiLU()\n",
       "  )\n",
       "  (dc_gate): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (nm_gate): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用minGPT\n",
    "\n",
    "\n",
    "# model_config.model_type = model_type\n",
    "# model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "# model_config.block_size = train_dataset.get_block_size()\n",
    "# model = GPT(model_config, types=\"nm\")\n",
    "# 如果要关掉记忆模块，令types=None即可\n",
    "model, _ = GPT.from_pretrained(model_type)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a93934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 8 1024\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "from dynamic_cheatsheet import DynamicCheatsheetMemory\n",
    "from dynamic_cheatsheet.config_loader import load_config\n",
    "\n",
    "cfg = load_config(\"dynamic_cheatsheet/config.yaml\")\n",
    "dc_cfg = asdict(cfg.dc)\n",
    "\n",
    "# hidden_dim 必须 = GPT 的 n_embd（gpt2 是 768）\n",
    "hidden_dim = model.transformer.wpe.embedding_dim\n",
    "dynamic_cheatsheet = DynamicCheatsheetMemory(dc_cfg, hidden_dim)\n",
    "print(hidden_dim, dynamic_cheatsheet.dc_len, dynamic_cheatsheet.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with dc: 7.205523490905762\n",
      "loss w/ zero: 7.8425421714782715\n"
     ]
    }
   ],
   "source": [
    "# # 最简单 batch_size = 1\n",
    "# prompt = \"Game of 24: numbers are 3, 3, 8, 8. Output ONE expression equals 24.\"\n",
    "\n",
    "prompts = [\n",
    "    \"Game of 24: numbers are 3, 3, 8, 8. Output ONE expression equals 24.\",\n",
    "    \"Game of 24: numbers are 1, 5, 5, 5. Output ONE expression equals 24.\",\n",
    "    \"Game of 24: numbers are 4, 4, 10, 10. Output ONE expression equals 24.\",\n",
    "]\n",
    "\n",
    "encoded = tokenizer_hf(prompts, return_tensors='pt')\n",
    "idx = encoded['input_ids'].to(device) # (B,T) LongTensor\n",
    "B, T = idx.shape\n",
    "\n",
    "query_texts = [prompts]  # batch_size = 1\n",
    "dc_memory, dbg = dynamic_cheatsheet.retrieve(query_texts, batch_size=B, device=device)\n",
    "\n",
    "# targets：自回归 LM loss（版本 A：不手动 shift，交给 model 内部）\n",
    "targets = idx.clone()\n",
    "targets[:, 0] = -1  # 可选：不算第一个 token 的 loss（看你 loss 是否 ignore_index=-1）\n",
    "# target 也来自 dataset，根据dataset的定义界定是否需要像idx一样处理\n",
    "\n",
    "# logits, loss = model(idx, targets=targets, dc_memory=dc_memory)\n",
    "# # 测试dc是否真的起作用\n",
    "# dc_zero = torch.zeros_like(dc_memory)\n",
    "# logits0, loss0 = model(idx, targets=targets, dc_memory=dc_zero)\n",
    "# print(\"loss with dc:\", loss.item())\n",
    "# print(\"loss w/ zero:\", loss0.item())\n",
    "\n",
    "# 4) forward（⚠️ 前提：你的 model.forward 接受 dc_memory）\n",
    "logits, loss = model(idx, targets=targets, dc_memory=dc_memory)\n",
    "print(\"logits:\", logits.shape, \"loss:\", loss.item() if loss is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ec8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "from minGPT.mingpt.trainer import Trainer\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 2000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)\n",
    "# train dataset的定义参看 minGPT/mingpt/train.py要求的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结部分参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "if getattr(model, \"neural_memory\", None) is not None:\n",
    "    for p in model.neural_memory.parameters():\n",
    "        p.requires_grad = True\n",
    "for block in model.transformer.h:\n",
    "    if hasattr(block, \"dc_gate\"):\n",
    "        for p in block.dc_gate.parameters():\n",
    "            p.requires_grad = True\n",
    "    if hasattr(block, \"gate\"):\n",
    "        for p in block.gate.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "trainer.run(dc_memory = None)\n",
    "# 这里的dc_memory为None，在gpt模型中会变成全0张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc280bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用生成\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "dc_memory = dynamic_cheatsheet.retrieve(prompt, batchsize=1, device=device)\n",
    "# 这里的batchsize需要在源代码中修改，对齐维度\n",
    "encoded = tokenizer_hf(prompt, return_tensors='pt')\n",
    "idx2 = encoded['input_ids'].to(device) # (1,T) LongTensor\n",
    "target = ()\n",
    "# target 来自 dataset，根据dataset的定义界定是否需要像idx一样处理\n",
    "\n",
    "with torch.no_grad():\n",
    "    cat = model.generate(idx2, n, do_sample=False, dc_memory=dc_memory)[0]\n",
    "    out = tokenizer_hf.decode(cat.cpu().squeeze())\n",
    "    # 注意参数对齐\n",
    "dynamic_cheatsheet.update(out, device=device, max_entries=100)\n",
    "# 这里的max_entries和谁对齐？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以生成评估函数\n",
    "def evaluate_model(model, eval_dataset, dc_memory=None):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
