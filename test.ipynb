{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a417d120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"minGPT\")   # 让 mingpt 这个包可 import\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from minGPT.mingpt.model import GPT\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "from minGPT.mingpt.bpe import BPETokenizer\n",
    "\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66067e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1511554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    }
   ],
   "source": [
    "# 调用minGPT\n",
    "# 如果要关掉记忆模块，令types=None即可\n",
    "model, _ = GPT.from_pretrained(model_type, types=\"nm\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a93934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状： torch.Size([32, 1024])\n",
      "目标形状： torch.Size([32, 1024])\n",
      "prompt的类型 <class 'list'> 32\n",
      "prompt的第15个元素： Task: Reverse the string and remove vowels (a,e,i,o,u).\n",
      "Input: wcrwbjdqprjw\n",
      "Output:\n",
      "answer的类型 <class 'list'> 32\n",
      "answer的第15个元素： wjrpqdjbwrcw\n"
     ]
    }
   ],
   "source": [
    "from minGPT.mingpt.program_dataset import ProgramDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dynamic_cheatsheet import DynamicCheatsheetMemory\n",
    "from dynamic_cheatsheet.config_loader import load_config\n",
    "from dataclasses import asdict\n",
    "import random\n",
    "\n",
    "# 准备数据集\n",
    "train_dataset = ProgramDataset(\n",
    "    jsonl_path=\"./data_reverse_dropvowel/train.jsonl\",\n",
    "    block_size=1024,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 得到一个batch的数据\n",
    "def get_one_batch(dataset, batch_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    prompts = []\n",
    "    answers = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        i = random.randrange(len(dataset))\n",
    "        idx, target, prompt, answer = dataset[i]\n",
    "        x.append(idx)\n",
    "        y.append(target)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    \n",
    "    x = torch.stack(x, dim=0)\n",
    "    y = torch.stack(y, dim=0)\n",
    "    \n",
    "    return x, y, prompts, answers\n",
    "\n",
    "x, y, prompts, answers = get_one_batch(train_dataset, batch_size=32)\n",
    "\n",
    "print(\"输入形状：\", x.shape)\n",
    "print(\"目标形状：\", y.shape)\n",
    "print(\"prompt的类型\", type(prompts), len(prompts))\n",
    "print(\"prompt的第15个元素：\", prompts[15])\n",
    "print(\"answer的类型\", type(answers), len(answers))\n",
    "print(\"answer的第15个元素：\", answers[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bdea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 8 1024\n",
      "dc_memory type: <class 'torch.Tensor'>\n",
      "dc_memory shape: torch.Size([32, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"dynamic_cheatsheet/config.yaml\")\n",
    "# hidden_dim 必须 = GPT 的 n_embd（gpt2 是 768）\n",
    "hidden_dim = model.transformer.wpe.embedding_dim\n",
    "dynamic_cheatsheet = DynamicCheatsheetMemory(asdict(cfg.dc), hidden_dim)\n",
    "print(hidden_dim, dynamic_cheatsheet.dc_len, dynamic_cheatsheet.embed_dim)\n",
    "\n",
    "dc_memory, _ = dynamic_cheatsheet.retrieve(prompts, batch_size=32, device=device)\n",
    "print(\"dc_memory type:\", type(dc_memory))\n",
    "print(\"dc_memory shape:\", dc_memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f3da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 0，损失 5.9561\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Missing API key: please set environment variable DASHSCOPE_API_KEY",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m     gen_text = tokenizer.decode(gen_ids[\u001b[32m0\u001b[39m])\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mdynamic_cheatsheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m评估模式下，迭代 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m，损失 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\anaconda3\\envs\\test\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\vscode\\PRML\\PRML\\dynamic_cheatsheet\\cheatsheet_memory.py:130\u001b[39m, in \u001b[36mDynamicCheatsheetMemory.update\u001b[39m\u001b[34m(self, window_text, device, max_entries)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_entries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# 如果没有配置，默认 2\u001b[39;00m\n\u001b[32m    128\u001b[39m     max_entries = \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m raw_entries = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_entries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_entries:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\vscode\\PRML\\PRML\\dynamic_cheatsheet\\llm_client.py:153\u001b[39m, in \u001b[36mLLMClient.generate_entries\u001b[39m\u001b[34m(self, window_text, max_entries)\u001b[39m\n\u001b[32m    147\u001b[39m user_prompt = build_user_prompt(window_text, max_entries=max_entries)\n\u001b[32m    148\u001b[39m messages = [\n\u001b[32m    149\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[32m    150\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_prompt},\n\u001b[32m    151\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_qwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m json_text = _extract_json_array(text)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_text:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\29072\\vscode\\PRML\\PRML\\dynamic_cheatsheet\\llm_client.py:124\u001b[39m, in \u001b[36mLLMClient._call_qwen\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    122\u001b[39m api_key = os.getenv(\u001b[38;5;28mself\u001b[39m.api_key_env)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    125\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing API key: please set environment variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.api_key_env\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    128\u001b[39m resp = dashscope.Generation.call(\n\u001b[32m    129\u001b[39m     api_key=api_key,\n\u001b[32m    130\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m.max_tokens,\n\u001b[32m    135\u001b[39m )\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Missing API key: please set environment variable DASHSCOPE_API_KEY"
     ]
    }
   ],
   "source": [
    "# 冻结部分参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "if getattr(model, \"neural_memory\", None) is not None:\n",
    "    for p in model.neural_memory.parameters():\n",
    "        p.requires_grad = True\n",
    "if hasattr(model, \"dc_gate\"):\n",
    "    for p in model.dc_gate.parameters():\n",
    "        p.requires_grad = True\n",
    "if hasattr(model, \"nm_gate\"):\n",
    "    for p in model.nm_gate.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# 获取可训练参数及其名称\n",
    "trainable_params = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "# print(\"Trainable param groups:\")\n",
    "# for n, p in trainable_params:\n",
    "#     print(f\"  {n:60s} {p.numel()}\")\n",
    "# print(\"Total trainable:\", sum(p.numel() for _, p in trainable_params))\n",
    "\n",
    "# 只传递参数对象给优化器\n",
    "optimizer = torch.optim.AdamW([p for _, p in trainable_params], lr=3e-4)\n",
    "\n",
    "max_iters = 3\n",
    "batch_size = 2\n",
    "EOS_ID = 50256\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    x, y, prompts, answers = get_one_batch(train_dataset, batch_size=batch_size)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dc_memory, _ = dynamic_cheatsheet.retrieve(prompts, batch_size=batch_size, device=device)\n",
    "    \n",
    "    model.train()\n",
    "    logits, loss = model(x, y, dc_memory = dc_memory)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"迭代 {iter}，损失 {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(batch_size):\n",
    "        prompt_idx = tokenizer(prompts[i])[0].long().unsqueeze(0).to(device)\n",
    "        gen_ids = model.generate(\n",
    "            prompt_idx,\n",
    "            max_new_tokens=30,\n",
    "            temperature=1.0,\n",
    "            do_sample=False,\n",
    "            top_k=None,\n",
    "            dc_memory=dc_memory[i:i+1],\n",
    "            eos_token_id=EOS_ID,\n",
    "            return_only_generated=False,\n",
    "        )\n",
    "        gen_text = tokenizer.decode(gen_ids[0])\n",
    "        with torch.no_grad():\n",
    "            dynamic_cheatsheet.update(gen_text, device=device)\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"评估模式下，迭代 {iter}，损失 {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结部分参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "if getattr(model, \"neural_memory\", None) is not None:\n",
    "    for p in model.neural_memory.parameters():\n",
    "        p.requires_grad = True\n",
    "for block in model.transformer.h:\n",
    "    if hasattr(block, \"dc_gate\"):\n",
    "        for p in block.dc_gate.parameters():\n",
    "            p.requires_grad = True\n",
    "    if hasattr(block, \"gate\"):\n",
    "        for p in block.gate.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "trainer.run()\n",
    "# 这里的dc_memory为None，在gpt模型中会变成全0张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc280bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用生成\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "dc_memory = dynamic_cheatsheet.retrieve(prompt, batchsize=1, device=device)\n",
    "# 这里的batchsize需要在源代码中修改，对齐维度\n",
    "encoded = tokenizer_hf(prompt, return_tensors='pt')\n",
    "idx2 = encoded['input_ids'].to(device) # (1,T) LongTensor\n",
    "target = ()\n",
    "# target 来自 dataset，根据dataset的定义界定是否需要像idx一样处理\n",
    "\n",
    "with torch.no_grad():\n",
    "    cat = model.generate(idx2, n, do_sample=False, dc_memory=dc_memory)[0]\n",
    "    out = tokenizer_hf.decode(cat.cpu().squeeze())\n",
    "    # 注意参数对齐\n",
    "dynamic_cheatsheet.update(out, device=device, max_entries=100)\n",
    "# 这里的max_entries和谁对齐？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以生成评估函数\n",
    "def evaluate_model(model, eval_dataset, dc_memory=None):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
